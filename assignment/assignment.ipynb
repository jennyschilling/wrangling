{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "13ad028b-72b7-43ed-aa78-96fd4e518040",
      "metadata": {
        "id": "13ad028b-72b7-43ed-aa78-96fd4e518040"
      },
      "source": [
        "# Assignment: Data Wrangling\n",
        "## `! git clone https://github.com/DS3001/wrangling`\n",
        "## Do Q2, and one of Q1 or Q3."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "rjGHGu3JBf8n"
      },
      "id": "rjGHGu3JBf8n",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/jennyschilling/wrangling"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGE6ditoGAfy",
        "outputId": "fd7e2819-67e4-43d2-fb92-92a8bb0b0943"
      },
      "id": "aGE6ditoGAfy",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'wrangling'...\n",
            "remote: Enumerating objects: 112, done.\u001b[K\n",
            "remote: Counting objects: 100% (72/72), done.\u001b[K\n",
            "remote: Compressing objects: 100% (60/60), done.\u001b[K\n",
            "remote: Total 112 (delta 53), reused 18 (delta 12), pack-reused 40\u001b[K\n",
            "Receiving objects: 100% (112/112), 6.26 MiB | 12.17 MiB/s, done.\n",
            "Resolving deltas: 100% (59/59), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1"
      ],
      "metadata": {
        "id": "PzR8N3KOIEKV"
      },
      "id": "PzR8N3KOIEKV"
    },
    {
      "cell_type": "markdown",
      "id": "5735a4d4-8be8-433a-a351-70eb8002e632",
      "metadata": {
        "id": "5735a4d4-8be8-433a-a351-70eb8002e632"
      },
      "source": [
        "**Q1.** Open the \"tidy_data.pdf\" document in the repo, which is a paper called Tidy Data by Hadley Wickham.\n",
        "\n",
        "  1. Read the abstract. What is this paper about?\n",
        "  > The importance of the proccess of data cleaning cannot be understated, so we want to make it as easy and efficient as we can. This paper addresses this challenge of data cleaning and organization, and introduces the concept of \"tidy data\" that simplifies this process.\n",
        "  2. Read the introduction. What is the \"tidy data standard\" intended to accomplish?\n",
        "  > When analysing real-world datasets, we often see that the limited constraints and regulations on the construction of datasets leave them crafted in weird and incosistent ways. The \"tidy data standard\" is intended to give a consistent method for organizing data to make data analysis and statistical modeling more efficient, eliminating the need for data scientists to \"reinvent the wheel\" every time they go to clean data.\n",
        "  3. Read the intro to section 2. What does this sentence mean: \"Like families, tidy datasets are all alike but every messy dataset is messy in its own way.\" What does this sentence mean: \"For a given dataset, itâ€™s usually easy to figure out what are observations and what are variables, but it is surprisingly difficult to precisely define variables and observations in general.\"\n",
        "  > The first sentence means that since tidy datasets follow a uniform structure, they are standardized and therefore easy to predict the structure of and clean/manipulate. In contrast, every messy dataset has its own unique set of issues, and so we can't predict what these issues will be or what the extend of the issues are when we choose to use the dataset. Going along with this, the second sentence is talking about how identifying the variables and observations is intuitive in most datasets, but actually defining them can be challenging. I.e., it is easier to describe the relationships between actual variables than between rows of a datset, and to make comparisons between groups of observations than just groups of columns.\n",
        "  4. Read Section 2.2. How does Wickham define values, variables, and observations?\n",
        "  > Wickham defines values as the actual data in our dataset, variables as all those values that measure the same underlying attribute, and observations as all the values that are measured on the same unit across attributes.\n",
        "  5. How is \"Tidy Data\" defined in section 2.3?\n",
        "  > Tidy Data is defined in section 2.3 as a standardized way of mapping the meaning of a dataset to its structure. This standardized method is characterized by each variable forming a column,  each observation forming a row, and each type of observational unit forming a table.\n",
        "  6. Read the intro to Section 3 and Section 3.1. What are the 5 most common problems with messy datasets? Why are the data in Table 4 messy? What is \"melting\" a dataset?\n",
        "  > The five most common problems with messy datasets are: (1) Column headers are values, not variable names. (2) Multiple variables are stored in one column. (3) Variables are stored in both rows and columns. (4) Multiple types of observational units are stored in the same table. (5) A single observational unit is stored in multiple tables.\n",
        "  > In Table 4, the data is designed for presentation, with variables forming both the rows and columns, and the column headers being the values, not variable names. This table can be tidied by \"melting\" it, which includes turning the columns, or colvars, into rows.\n",
        "  7. Why, specifically, is table 11 messy but table 12 tidy and \"molten\"?\n",
        "  > Table 11 is messy because it presents the days as column headers, representing values. Table 12 is molten because although it converts these days into a single date variable, it still is unorganized in terms of tidiness since the element variable represents variable names rather than actual values.\n",
        "  8. Read Section 6. What is the \"chicken-and-egg\" problem with focusing on tidy data? What does Wickham hope happens in the future with further work on the subject of data wrangling?\n",
        "  > The \"chicken-and-egg\" problem with tidy data is that our \"tidy data\" is only really as useful as the tools that we have to work with it, so the application of tidy tools is really just superficial and, in a sense, orchestrated by marketing schemes. Wickham hopes that a more expansive approach to data cleaning comes to fruition, as well as a vaster and sturdier framework of the ideas and tools that make up the domain of data science."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2"
      ],
      "metadata": {
        "id": "Nt2GjtarIAy8"
      },
      "id": "Nt2GjtarIAy8"
    },
    {
      "cell_type": "markdown",
      "id": "da879ea7-8aac-48a3-b6c2-daea56d2e072",
      "metadata": {
        "id": "da879ea7-8aac-48a3-b6c2-daea56d2e072"
      },
      "source": [
        "**Q2.** This question provides some practice cleaning variables which have common problems.\n",
        "1. Numeric variable: For `./data/airbnb_hw.csv`, clean the `Price` variable as well as you can, and explain the choices you make. How many missing values do you end up with? (Hint: What happens to the formatting when a price goes over 999 dollars, say from 675 to 1,112?)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The issue with this variable is that when the price goes over 999, commas are introduced to the string, so we can't convert the prices from strings to floats/numerical values. To fix this, we can just remove all the commas from the prices and then convert the prices from strings to floats."
      ],
      "metadata": {
        "id": "I_KeO1PIJyQa"
      },
      "id": "I_KeO1PIJyQa"
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 1: Numeric Variable\n",
        "df = pd.read_csv('wrangling/assignment/data/airbnb_hw.csv',low_memory=False)\n",
        "prices = df['Price']\n",
        "prices = prices.str.replace(',', '')\n",
        "prices = pd.to_numeric(prices)\n",
        "df['Price'] = prices\n",
        "del prices\n",
        "# print(df['Price'].unique())"
      ],
      "metadata": {
        "id": "KNnh5D4ICH5z"
      },
      "id": "KNnh5D4ICH5z",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Categorical variable: For the `./data/sharks.csv` data covered in the lecture, clean the \"Type\" variable as well as you can, and explain the choices you make.\n",
        "\n",
        "The \"Type\" variable has several issues within its rows. Firstly, the types \"Sea Disaster\", \"Watercraft\", \"Boat\", and \"Boating\" could be put into the same category. Similarly, \"Invalid\", \"Questionable\", \"Unconfirmed\", \"Unverified\", and \"Under investigation\" could all be put into the same category as nan objects since they're all unknown attack types and may as well be void. There is also a category with just one entry called \"Boatomg\", which is likely a typo and should be put into the category with water accidents."
      ],
      "metadata": {
        "id": "y8sL9lYpLnGQ"
      },
      "id": "y8sL9lYpLnGQ"
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 2: Categorical Variable\n",
        "df = pd.read_csv('wrangling/assignment/data/sharks.csv', low_memory=False)\n",
        "types = df['Type']\n",
        "types = types.replace(['Watercraft', 'Boat','Boating','Boatomg'], 'Sea Disaster')\n",
        "types = types.replace(['Invalid', 'Questionable','Unconfirmed','Unverified','Under investigation'], np.nan)\n",
        "df['Type'] = types\n",
        "del types\n",
        "# df['Type'].value_counts()"
      ],
      "metadata": {
        "id": "T1zDbzc4MFIX"
      },
      "id": "T1zDbzc4MFIX",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Dummy variable: For the pretrial data covered in the lecture, clean the `WhetherDefendantWasReleasedPretrial` variable as well as you can, and, in particular, replace missing values with `np.nan`.\n"
      ],
      "metadata": {
        "id": "wo7PS2RpLzGv"
      },
      "id": "wo7PS2RpLzGv"
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 3: Dummy Variable\n",
        "url = 'http://www.vcsc.virginia.gov/pretrialdataproject/October%202017%20Cohort_Virginia%20Pretrial%20Data%20Project_Deidentified%20FINAL%20Update_10272021.csv'\n",
        "df = pd.read_csv(url, low_memory=False)\n",
        "temp = df['WhetherDefendantWasReleasedPretrial']\n",
        "temp = temp.replace(9, np.nan) # 9's are unclear\n",
        "df['WhetherDefendantWasReleasedPretrial'] = temp\n",
        "del temp"
      ],
      "metadata": {
        "id": "Ctm2ZqiyMBpZ"
      },
      "id": "Ctm2ZqiyMBpZ",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Missing values, not at random: For the pretrial data covered in the lecture, clean the `ImposedSentenceAllChargeInContactEvent` variable as well as you can, and explain the choices you make. (Hint: Look at the `SentenceTypeAllChargesAtConvictionInContactEvent` variable.)"
      ],
      "metadata": {
        "id": "3yTaVmJjL1Jv"
      },
      "id": "3yTaVmJjL1Jv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The handbook defines this variable as \"the total imposed sentence term for any charges in the October 2017 contact event resulting in a conviction (in months).\" When looking at all the different entries for this variable (which should be a numeric value representing the number of months of conviction), we can see that several entries are empty. Looking at the corresponding variable, \"SentenceTypeAllChargesAtConvictionInContactEvent,\" we note that all types with a 4 were dismissed/overturned, and types with a 9 are not applicable. So, we have to replace those entries with a SentenceType of 4 to 0 months in the ImposedSentence column, and those with a SentenceType of 9  as nan objects in the ImposedSentence column. Other than this, I did not find any other empty entries or unclean data like negative values."
      ],
      "metadata": {
        "id": "aMZbgfc6MIAg"
      },
      "id": "aMZbgfc6MIAg"
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 4: Missing values, not at random\n",
        "sentence_type = df['SentenceTypeAllChargesAtConvictionInContactEvent']\n",
        "imposed_sentence = df['ImposedSentenceAllChargeInContactEvent']\n",
        "imposed_sentence = pd.to_numeric(imposed_sentence, errors='coerce')\n",
        "imposed_sentence = imposed_sentence.mask(sentence_type == 4, 0)\n",
        "imposed_sentence = imposed_sentence.mask(sentence_type == 9, np.nan)\n",
        "df['ImposedSentenceAllChargeInContactEvent'] = imposed_sentence\n",
        "del imposed_sentence"
      ],
      "metadata": {
        "id": "Na7iU0DGMNJe"
      },
      "id": "Na7iU0DGMNJe",
      "execution_count": 23,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}